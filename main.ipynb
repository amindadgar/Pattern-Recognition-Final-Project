{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft margin SVM\n",
    "this part we are going to implement soft margin svm. We're using cvxopt library to solve quadratic equations in support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formulas and equations\n",
    "First we will speak of the CVXOPT library. in this library to solve a quadratic equation, equations must be in the form as equation \\ref{eq:quadratic-cvxopt-form}\n",
    "\\begin{equation} \n",
    "\\begin{aligned}\n",
    "&min \\: \\frac{1}{2} x^T P x + q^Tx \\\\\n",
    "&subject \\; to \\; Gx \\leq h \\\\\n",
    "& Ax=b\n",
    "\\end{aligned}\n",
    "\\label{eq:quadratic-cvxopt-form}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dual problem equations in support vector machines are as\n",
    "\\begin{equation} \\label{eq:dual-problem-expression}\n",
    "max \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y_i y_j \\alpha_i \\alpha_j x_{i}^{T} x_j\n",
    "\\end{equation}\n",
    "And if we write $H$ as $H_{i,j} = y_i y_j x_{i}^{T} x_j$\n",
    "the optimazation equation will be as \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&max \\: \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\alpha^T H \\alpha \\\\\n",
    "& subject \\; to \\; 0 \\leq \\alpha_i \\leq C \\\\\n",
    "& \\sum_{i}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Note that $H$ will be our gram matrix. and if we want to convert our problem to the notation of CVXOPT library we can write\n",
    "\n",
    " $P$ stands for H with the shape $m \\times m$, $q$ stands for -1 vector with shape of $m \\times 1$, $G$ stands for a digonal matrix with -1 on the diagonal and the shape of $m \\times m$, $h$ stands for a zero vector with the shape of $m \\times 1$, $A$ stands for the label vector y with the shape of $m \\times 1$ and $b$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the maximum problem into minimum we can multilpy all equations with a $-1$ \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& min \\: \\frac{1}{2} \\alpha^T H \\alpha - 1^T \\alpha_i \\\\\n",
    "& subject \\; to \\; \\alpha_i \\leq C \\\\\n",
    "&  -\\alpha_i \\leq 0 \\\\\n",
    "& \\sum_{i}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "also we can rewrite the last equation as below, meaning all lables ($y$) are in one vector\n",
    "\\begin{equation}\n",
    "y^T \\alpha = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation\n",
    "Now we're going to implement soft margin svm with an example.  The data are as follows\n",
    "\\begin{equation}\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "3 & 1 & 2 & 6 & 7 & 5 & 2 \\\\\n",
    "4 & 4 & 3 & -1 & -1 & -3 & 4\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "And the labels are \n",
    "\\begin{equation}\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "because the dimension of samples are 2 and we have two constraints $G$ and $h$, we can write these constraints as \n",
    "\\begin{equation}\n",
    "G = \n",
    "\\begin{pmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "C \\\\\n",
    "C\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_soft_margin(X, y, C):\n",
    "    \"\"\"\n",
    "    implementation of soft margin svm\n",
    "    we didn't use a kernel function we just made a linear inner product of data\n",
    "    \n",
    "    INPUTS:\n",
    "    --------\n",
    "    X: the numpy matrix of input vectors, shape is m*n meaning m is the dimension and n is data sample size\n",
    "    y: numpy vector of labels, shape is n*1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m,n = X.shape\n",
    "    \n",
    "    ## multiplying the labels with inputs\n",
    "    X_dash = y * X\n",
    "\n",
    "    ############### LINEAR product ###############\n",
    "    ## inner product \n",
    "    ## with this the labels will be multiplied twice as in the dual problem equation\n",
    "    H = np.dot(X_dash, X_dash.T)\n",
    "    \n",
    "    ## convert data into cvxopt format\n",
    "    P = cvxopt.matrix(H)\n",
    "    q = cvxopt.matrix(-np.ones(m))\n",
    "    G = cvxopt.matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "    A = cvxopt.matrix(y.reshape(1, -1))\n",
    "    b = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "    ## solve the qaudratic equations\n",
    "    answer = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "    ## the lagrangian coefficients (named as alpha)\n",
    "    alphas = np.array(answer['x'])\n",
    "\n",
    "    ## computing parameters of the line equation\n",
    "    w = ((y * alphas).T @ X).reshape(-1,1)\n",
    "    S = (alphas > 1e-4).flatten()\n",
    "    b = y[S] - np.dot(X[S], w)\n",
    "\n",
    "    #Display results\n",
    "    print('Alphas = ',alphas[alphas > 1e-4])\n",
    "    print('w = ', w.flatten())\n",
    "    print('b = ', b[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5556e+01 -2.5999e+02  3e+02  1e-01  2e-14\n",
      " 1: -1.8050e+01 -3.9497e+01  2e+01  4e-03  1e-14\n",
      " 2: -2.1437e+01 -2.3412e+01  2e+00  3e-04  2e-14\n",
      " 3: -2.2496e+01 -2.2997e+01  5e-01  3e-05  1e-14\n",
      " 4: -2.2561e+01 -2.2568e+01  6e-03  3e-07  2e-14\n",
      " 5: -2.2562e+01 -2.2563e+01  6e-05  3e-09  2e-14\n",
      " 6: -2.2562e+01 -2.2563e+01  6e-07  3e-11  1e-14\n",
      "Optimal solution found.\n",
      "Alphas =  [4.9999998  6.31250013 1.31249982 9.99999997]\n",
      "w =  [ 0.25000001 -0.25000001]\n",
      "b =  [-0.74999997]\n"
     ]
    }
   ],
   "source": [
    "## dataset\n",
    "X = np.array([[3,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3],[2,4]], dtype=float)\n",
    "y = np.array([-1,-1, -1, 1, 1 , 1, 1 ], dtype=float)\n",
    "\n",
    "## define C \n",
    "C = 10\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "svm_soft_margin(X, y, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Support Vector Machine (TWSVM)\n",
    "In this type of svm called twin support vector machines we are going to have to non-parallel lines. This is a binary classifier as SVM and to classify each data we need to find out that the data point is close to which of the hyperplains. <br>\n",
    "To demonstrate the equation for this type of svm we can write them as below.\n",
    "\\begin{align*}\n",
    "w_1^T x+b_1&=0          &  w_2^Tx+b_2&=0\n",
    "\\end{align*}\n",
    "And we need to solve quadratic equations as below\n",
    "\\begin{equation}\n",
    "min \\; \\frac{1}{2} (Aw_1 + e_1 b_1)^T(Aw_1 + e_1 b_1) + c_1 e_2^T \\xi_2 \\\\\n",
    "subject \\; to \\; -(Bw_1 + e_2 b_1) + \\xi_2 \\geq e_2 \\\\\n",
    "\\xi_2 \\geq 0\n",
    "\\end{equation}\n",
    "and for the second line\n",
    "\\begin{equation}\n",
    "min \\; \\frac{1}{2} (Bw_2 + e_2 b_2)^T(Bw_2 + e_2 b_2) + c_2 e_1^T \\xi_1 \\\\\n",
    "subject \\; to \\; Aw_2 + e_1 b_2 + \\xi_1 \\geq e_1 \\\\\n",
    "\\xi_1 \\geq 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go and calculate them we using wolfe dual method, can find the answers as below\n",
    "\\begin{equation}\n",
    "max \\; e_2^T \\alpha - \\frac{1}{2} \\alpha^T G(H^TH)^{-1} G^T \\alpha \\\\\n",
    "subject \\; to \\; 0 \\leq \\alpha \\leq c_1\n",
    "\\end{equation}\n",
    "and for the second line \n",
    "\\begin{equation}\n",
    "max \\; e_1^T \\beta - \\frac{1}{2} \\beta^T H(G^TG)^{-1} H^T \\beta \\\\\n",
    "subject \\; to \\; 0 \\leq \\beta \\leq c_2\n",
    "\\end{equation}\n",
    "while $G=[A \\;\\; e_1]$, $H=[B \\;\\; e_2]$, $e_1$ and $e_2$ are the column vector of one with the right dimension(data dimension). Also the lagrange coefficients are $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find the weight and bias of the hyperplains, we can derive it from equations below\n",
    "\\begin{align*}\n",
    "v_1&=-(H^TH)^{-1}G^T\\alpha      &    where \\;\\;\\; v_1&=[w_1 \\;\\; b_1]^T \\\\\n",
    "v_2&=-(G^TG)^{-1}H^T\\beta      &    where \\;\\;\\; v_2&=[w_2 \\;\\; b_2]^T\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find the class for each data we can use the equation below to find the closest hyperplain for a data\n",
    "\\begin{equation}\n",
    "Class \\;\\; K = argmin_{k=1,2} \\frac{|w_k^Tx+b_k|}{||w_k||}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation\n",
    "We set $m=2$, meaning the dimension of data is two. then we can derive $G$ and $h$ matrices for lagrangian coefficients are as below\n",
    "\\begin{equation}\n",
    "G_1 = \n",
    "\\begin{pmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_1 = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "Also $h_2$ and $G_2$ are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3407e-15 -4.0000e+01  9e+01  1e-01  2e-15\n",
      " 1:  6.3531e-17 -7.3007e-01  1e+00  1e-03  2e-15\n",
      " 2:  6.2971e-19 -7.3014e-03  1e-02  1e-05  3e-15\n",
      " 3:  6.2978e-21 -7.3014e-05  1e-04  1e-07  2e-15\n",
      " 4:  6.2978e-23 -7.3014e-07  1e-06  1e-09  2e-15\n",
      " 5:  6.2978e-25 -7.3014e-09  1e-08  1e-11  3e-15\n",
      "Optimal solution found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,8) (4,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7612/1654754882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0malphas1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0malphas1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,8) (4,1) "
     ]
    }
   ],
   "source": [
    "c1 = 10\n",
    "c2 = 10\n",
    "\n",
    "data_A = np.array([[3,4],[1.5,4],[2,3],[6,-1],[7,-1],[5,-3],[2,4]], dtype=float)\n",
    "y1 = np.array([1,1, 1, 1, 1 , 1, 1], dtype=float)\n",
    "\n",
    "data_A = np.array([[1,1], [1,0], [0,1]])\n",
    "y1 = np.array([1, 1, 1], dtype=float)\n",
    "y1 = y1.reshape(-1,1)\n",
    "\n",
    "y1 = np.concatenate((y1, [[1]]))\n",
    "\n",
    "data_B = np.array([[-1, 5], [-2, 6], [-3, 8], [-1.5, 5], [0, 2], [0, -2], [-1, -3]], dtype=float)\n",
    "y2 = np.array([-1,-1, -1, -1, -1 , -1, -1], dtype=float)\n",
    "# data_B = np.array([[0,0], [-1, -1], [-1, 0]])\n",
    "# y2 = np.array([-1, -1, -1], dtype= float)\n",
    "y2 = y2.reshape(-1,1)\n",
    "\n",
    "y2 = np.concatenate((y2, [[-1]]))\n",
    "\n",
    "m1, n1 = data_A.shape\n",
    "m2, n2 = data_B.shape\n",
    "m2 += 1\n",
    "m1 += 1\n",
    "\n",
    "e1 = np.ones(n1, dtype=float)\n",
    "e2 = np.ones(n2, dtype=float)\n",
    "\n",
    "## y1 and y2 are the labels and must be multiplied\n",
    "G =  y1 * np.concatenate(( data_A , [e1]), axis=0)\n",
    "H = y2 * np.concatenate((data_B , [e2]), axis=0)\n",
    "\n",
    "## for each hyperplain\n",
    "## the equations shows the maximum, in order to find the minimum a -1 is multiplied to each equation\n",
    "P1 = -1* G.dot(np.linalg.inv(np.dot(H.T, H))).dot(G.T)\n",
    "P2 = -1* H.dot(np.linalg.inv(np.dot(G.T, G))).dot(H.T)\n",
    "\n",
    "## convert to cvxopt matrix\n",
    "P1 = cvxopt.matrix(P1)\n",
    "P2 = cvxopt.matrix(P2)\n",
    "\n",
    "q1 = cvxopt.matrix(-np.ones(m1))\n",
    "q2 = cvxopt.matrix(-np.ones(m2))\n",
    "\n",
    "G1 = cvxopt.matrix(np.vstack((np.eye(m1)*-1,np.eye(m1))))\n",
    "h1 = cvxopt.matrix(np.hstack((np.zeros(m1), np.ones(m1) * c1)))\n",
    "\n",
    "G2 = cvxopt.matrix(np.vstack((np.eye(m2)*-1,np.eye(m2))))\n",
    "h2 = cvxopt.matrix(np.hstack((np.zeros(m2), np.ones(m2) * c2)))\n",
    "\n",
    "## the other constraints\n",
    "\n",
    "A1 = cvxopt.matrix(y1.reshape(1, -1))\n",
    "b1 = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "A2 = cvxopt.matrix(y2.reshape(1, -1))\n",
    "b2 = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "answer1 = cvxopt.solvers.qp(P1, q1, G1, h1, A1, b1)\n",
    "# answer2 = cvxopt.solvers.qp(P2, q2, G2, h2, A2, b2)\n",
    "\n",
    "\n",
    "## the lagrangian coefficients (named as alpha)\n",
    "alphas1 = np.array(answer1['x'])\n",
    "\n",
    "v1 = -1 * np.linalg.inv((G.T @ G)).dot(H.T) * alphas1\n",
    "v1\n",
    "\n",
    "# ## computing parameters of the line equation\n",
    "# w1 = ((y1 * alphas).T @ G).reshape(-1,1)\n",
    "# S1 = (alphas > 1e-4).flatten()\n",
    "# b1 = y1[S1] - np.dot(G, w1)\n",
    "\n",
    "# print(\"Answer 1\")\n",
    "# print('Alphas = ',alphas[alphas > 1e-4])\n",
    "# print('w1 = ', w.flatten())\n",
    "# print('b1 = ', b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_A[:,0], data_A[:,1])\n",
    "plt.scatter(data_B[:,0], data_B[:,1])\n",
    "plt.legend(['Class 1', 'Class -1'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -5. ],\n",
       "       [ 2. , -6. ],\n",
       "       [ 3. , -8. ],\n",
       "       [ 1.5, -5. ],\n",
       "       [-0. , -2. ],\n",
       "       [-0. ,  2. ],\n",
       "       [ 1. ,  3. ],\n",
       "       [-1. , -1. ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# np.concatenate((data_A, [e1])) \n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-SVCR\n",
    "The main concept of K-SVCR proposed by Cecilio Angulo and Andreu Catala in 2003 was to classify more than two classses and it uses the idea of one-vs-one-vs-rest. The difference between this method and the one-vs-one or one-vs-rest is, it will use three classes in each step of support vector evaluation. First it calculates a support vector for class label $+1$, then it calculates another one for label $-1$ and at last for other classes it apply label $0$ for other classes. <br>\n",
    "The equation below would help to better understand this method.(Note that $N$ is the count of our data) \n",
    "\\begin{equation}\n",
    "  f(x_p) =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if $p=1,\\; 2, ..., n$}\\\\\n",
    "      -1 & \\text{if $p=n+1, \\; n+2, ..., n+m$}\\\\\n",
    "      0 & \\text{if $p=n+m+1, \\; n+m+2, ..., N$}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "And the equations for the optimization problem will be\n",
    "\\begin{equation}\n",
    "arg \\: min \\; \\frac{1}{2} ||w||^2\n",
    "\\end{equation}\n",
    "With the equation above we are maximazing the margin of each decesion boundary."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29b18f6f7436456ded269b7bb0d23499090a9a566016416baeeafcc627f8cc57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
