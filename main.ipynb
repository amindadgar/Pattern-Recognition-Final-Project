{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxopt\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft margin SVM\n",
    "this part we are going to implement soft margin svm. We're using cvxopt library to solve quadratic equations in support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## formulas and equations\n",
    "First we will speak of the CVXOPT library. in this library to solve a quadratic equation, equations must be in the form as equation \\ref{eq:quadratic-cvxopt-form}\n",
    "\\begin{equation} \n",
    "\\begin{aligned}\n",
    "&min \\: \\frac{1}{2} x^T P x + q^Tx \\\\\n",
    "&subject \\; to \\; Gx \\leq h \\\\\n",
    "& Ax=b\n",
    "\\end{aligned}\n",
    "\\label{eq:quadratic-cvxopt-form}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the dual problem equations in support vector machines are as\n",
    "\\begin{equation} \\label{eq:dual-problem-expression}\n",
    "max \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y_i y_j \\alpha_i \\alpha_j x_{i}^{T} x_j\n",
    "\\end{equation}\n",
    "And if we write $H$ as $H_{i,j} = y_i y_j x_{i}^{T} x_j$\n",
    "the optimazation equation will be as \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "&max \\: \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\alpha^T H \\alpha \\\\\n",
    "& subject \\; to \\; 0 \\leq \\alpha_i \\leq C \\\\\n",
    "& \\sum_{i}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Note that $H$ will be our gram matrix. and if we want to convert our problem to the notation of CVXOPT library we can write\n",
    "\n",
    " $P$ stands for H with the shape $m \\times m$, $q$ stands for -1 vector with shape of $m \\times 1$, $G$ stands for a digonal matrix with -1 on the diagonal and the shape of $m \\times m$, $h$ stands for a zero vector with the shape of $m \\times 1$, $A$ stands for the label vector y with the shape of $m \\times 1$ and $b$ is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert the maximum problem into minimum we can multilpy all equations with a $-1$ \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& min \\: \\frac{1}{2} \\alpha^T H \\alpha - 1^T \\alpha_i \\\\\n",
    "& subject \\; to \\; \\alpha_i \\leq C \\\\\n",
    "&  -\\alpha_i \\leq 0 \\\\\n",
    "& \\sum_{i}^{m} \\alpha_i y_i = 0\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "also we can rewrite the last equation as below, meaning all lables ($y$) are in one vector\n",
    "\\begin{equation}\n",
    "y^T \\alpha = 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation\n",
    "Now we're going to implement soft margin svm with an example.  The data are as follows\n",
    "\\begin{equation}\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "3 & 1 & 2 & 6 & 7 & 5 & 2 \\\\\n",
    "4 & 4 & 3 & -1 & -1 & -3 & 4\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "And the labels are \n",
    "\\begin{equation}\n",
    "Y = \n",
    "\\begin{pmatrix}\n",
    "-1 & -1 & -1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "because the dimension of samples are 2 and we have two constraints $G$ and $h$, we can write these constraints as \n",
    "\\begin{equation}\n",
    "G = \n",
    "\\begin{pmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "C \\\\\n",
    "C\n",
    "\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_soft_margin(X, y, C):\n",
    "    \"\"\"\n",
    "    implementation of soft margin svm\n",
    "    we didn't use a kernel function we just made a linear inner product of data\n",
    "    \n",
    "    INPUTS:\n",
    "    --------\n",
    "    X: the numpy matrix of input vectors, shape is m*n meaning m is the dimension and n is data sample size\n",
    "    y: numpy vector of labels, shape is n*1\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m,n = X.shape\n",
    "    \n",
    "    ## multiplying the labels with inputs\n",
    "    X_dash = y * X\n",
    "\n",
    "    ############### LINEAR product ###############\n",
    "    ## inner product \n",
    "    ## with this the labels will be multiplied twice as in the dual problem equation\n",
    "    H = np.dot(X_dash, X_dash.T)\n",
    "    \n",
    "    ## convert data into cvxopt format\n",
    "    P = cvxopt.matrix(H)\n",
    "    q = cvxopt.matrix(-np.ones(m))\n",
    "    G = cvxopt.matrix(np.vstack((np.eye(m)*-1,np.eye(m))))\n",
    "    h = cvxopt.matrix(np.hstack((np.zeros(m), np.ones(m) * C)))\n",
    "    A = cvxopt.matrix(y.reshape(1, -1))\n",
    "    b = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "    ## solve the qaudratic equations\n",
    "    answer = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "    ## the lagrangian coefficients (named as alpha)\n",
    "    alphas = np.array(answer['x'])\n",
    "\n",
    "    ## computing parameters of the line equation\n",
    "    w = ((y * alphas).T @ X).reshape(-1,1)\n",
    "    S = (alphas > 1e-4).flatten()\n",
    "    b = y[S] - np.dot(X[S], w)\n",
    "\n",
    "    #Display results\n",
    "    print('Alphas = ',alphas[alphas > 1e-4])\n",
    "    print('w = ', w.flatten())\n",
    "    print('b = ', b[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.5556e+01 -2.5999e+02  3e+02  1e-01  2e-14\n",
      " 1: -1.8050e+01 -3.9497e+01  2e+01  4e-03  1e-14\n",
      " 2: -2.1437e+01 -2.3412e+01  2e+00  3e-04  2e-14\n",
      " 3: -2.2496e+01 -2.2997e+01  5e-01  3e-05  1e-14\n",
      " 4: -2.2561e+01 -2.2568e+01  6e-03  3e-07  2e-14\n",
      " 5: -2.2562e+01 -2.2563e+01  6e-05  3e-09  2e-14\n",
      " 6: -2.2562e+01 -2.2563e+01  6e-07  3e-11  1e-14\n",
      "Optimal solution found.\n",
      "Alphas =  [4.9999998  6.31250013 1.31249982 9.99999997]\n",
      "w =  [ 0.25000001 -0.25000001]\n",
      "b =  [-0.74999997]\n"
     ]
    }
   ],
   "source": [
    "## dataset\n",
    "X = np.array([[3,4],[1,4],[2,3],[6,-1],[7,-1],[5,-3],[2,4]], dtype=float)\n",
    "y = np.array([-1,-1, -1, 1, 1 , 1, 1 ], dtype=float)\n",
    "\n",
    "## define C \n",
    "C = 10\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "svm_soft_margin(X, y, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Support Vector Machine (TWSVM)\n",
    "In this type of svm called twin support vector machines we are going to have to non-parallel lines. This is a binary classifier as SVM and to classify each data we need to find out that the data point is close to which of the hyperplains. <br>\n",
    "To demonstrate the equation for this type of svm we can write them as below.\n",
    "\\begin{align*}\n",
    "w_1^T x+b_1&=0          &  w_2^Tx+b_2&=0\n",
    "\\end{align*}\n",
    "And we need to solve quadratic equations as below\n",
    "\\begin{equation}\n",
    "min \\; \\frac{1}{2} (Aw_1 + e_1 b_1)^T(Aw_1 + e_1 b_1) + c_1 e_2^T \\xi_2 \\\\\n",
    "subject \\; to \\; -(Bw_1 + e_2 b_1) + \\xi_2 \\geq e_2 \\\\\n",
    "\\xi_2 \\geq 0\n",
    "\\end{equation}\n",
    "and for the second line\n",
    "\\begin{equation}\n",
    "min \\; \\frac{1}{2} (Bw_2 + e_2 b_2)^T(Bw_2 + e_2 b_2) + c_2 e_1^T \\xi_1 \\\\\n",
    "subject \\; to \\; Aw_2 + e_1 b_2 + \\xi_1 \\geq e_1 \\\\\n",
    "\\xi_1 \\geq 0\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we go and calculate them we using wolfe dual method, can find the answers as below\n",
    "\\begin{equation}\n",
    "max \\; e_2^T \\alpha - \\frac{1}{2} \\alpha^T G(H^TH)^{-1} G^T \\alpha \\\\\n",
    "subject \\; to \\; 0 \\leq \\alpha \\leq c_1\n",
    "\\end{equation}\n",
    "and for the second line \n",
    "\\begin{equation}\n",
    "max \\; e_1^T \\beta - \\frac{1}{2} \\beta^T H(G^TG)^{-1} H^T \\beta \\\\\n",
    "subject \\; to \\; 0 \\leq \\beta \\leq c_2\n",
    "\\end{equation}\n",
    "while $G=[A \\;\\; e_1]$, $H=[B \\;\\; e_2]$, $e_1$ and $e_2$ are the column vector of one with the right dimension(data dimension). Also the lagrange coefficients are $\\alpha$ and $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find the weight and bias of the hyperplains, we can derive it from equations below\n",
    "\\begin{align*}\n",
    "v_1&=-(H^TH)^{-1}G^T\\alpha      &    where \\;\\;\\; v_1&=[w_1 \\;\\; b_1]^T \\\\\n",
    "v_2&=-(G^TG)^{-1}H^T\\beta      &    where \\;\\;\\; v_2&=[w_2 \\;\\; b_2]^T\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to find the class for each data we can use the equation below to find the closest hyperplain for a data\n",
    "\\begin{equation}\n",
    "Class \\;\\; K = argmin_{k=1,2} \\frac{|w_k^Tx+b_k|}{||w_k||}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation\n",
    "We set $m=2$, meaning the dimension of data is two. then we can derive $G$ and $h$ matrices for lagrangian coefficients are as below\n",
    "\\begin{equation}\n",
    "G_1 = \n",
    "\\begin{pmatrix}\n",
    "-1 & 0 \\\\\n",
    "0 & -1 \\\\\n",
    "1 & 0 \\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "h_1 = \n",
    "\\begin{pmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "Also $h_2$ and $G_2$ are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.3407e-15 -4.0000e+01  9e+01  1e-01  2e-15\n",
      " 1:  6.3531e-17 -7.3007e-01  1e+00  1e-03  2e-15\n",
      " 2:  6.2971e-19 -7.3014e-03  1e-02  1e-05  3e-15\n",
      " 3:  6.2978e-21 -7.3014e-05  1e-04  1e-07  2e-15\n",
      " 4:  6.2978e-23 -7.3014e-07  1e-06  1e-09  2e-15\n",
      " 5:  6.2978e-25 -7.3014e-09  1e-08  1e-11  3e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "c1 = 10\n",
    "c2 = 10\n",
    "\n",
    "data_A = np.array([[3,4],[1.5,4],[2,3],[6,-1],[7,-1],[5,-3],[2,4]], dtype=float)\n",
    "y1 = np.array([1,1, 1, 1, 1 , 1, 1], dtype=float)\n",
    "\n",
    "data_A = np.array([[1,1], [1,0], [0,1]])\n",
    "y1 = np.array([1, 1, 1], dtype=float)\n",
    "y1 = y1.reshape(-1,1)\n",
    "\n",
    "y1 = np.concatenate((y1, [[1]]))\n",
    "\n",
    "data_B = np.array([[-1, 5], [-2, 6], [-3, 8], [-1.5, 5], [0, 2], [0, -2], [-1, -3]], dtype=float)\n",
    "y2 = np.array([-1,-1, -1, -1, -1 , -1, -1], dtype=float)\n",
    "# data_B = np.array([[0,0], [-1, -1], [-1, 0]])\n",
    "# y2 = np.array([-1, -1, -1], dtype= float)\n",
    "y2 = y2.reshape(-1,1)\n",
    "\n",
    "y2 = np.concatenate((y2, [[-1]]))\n",
    "\n",
    "m1, n1 = data_A.shape\n",
    "m2, n2 = data_B.shape\n",
    "m2 += 1\n",
    "m1 += 1\n",
    "\n",
    "e1 = np.ones(n1, dtype=float)\n",
    "e2 = np.ones(n2, dtype=float)\n",
    "\n",
    "## y1 and y2 are the labels and must be multiplied\n",
    "G =  y1 * np.concatenate(( data_A , [e1]), axis=0)\n",
    "H = y2 * np.concatenate((data_B , [e2]), axis=0)\n",
    "\n",
    "## for each hyperplain\n",
    "## the equations shows the maximum, in order to find the minimum a -1 is multiplied to each equation\n",
    "P1 = -1* G.dot(np.linalg.inv(np.dot(H.T, H))).dot(G.T)\n",
    "\n",
    "## convert to cvxopt matrix\n",
    "P1 = cvxopt.matrix(P1)\n",
    "\n",
    "q1 = cvxopt.matrix(-np.ones(m1))\n",
    "\n",
    "G1 = cvxopt.matrix(np.vstack((np.eye(m1)*-1,np.eye(m1))))\n",
    "h1 = cvxopt.matrix(np.hstack((np.zeros(m1), np.ones(m1) * c1)))\n",
    "\n",
    "\n",
    "## the other constraints\n",
    "\n",
    "A1 = cvxopt.matrix(y1.reshape(1, -1))\n",
    "b1 = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "answer1 = cvxopt.solvers.qp(P1, q1, G1, h1, A1, b1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1\n",
      "Alphas =  [6.02580322e-25 3.35252026e-26 1.61355036e-25]\n",
      "w1 =  [-6.63302389e-25 -1.23235751e-24]\n",
      "b1 =  [1.]\n"
     ]
    }
   ],
   "source": [
    "## the lagrangian coefficients (named as alpha)\n",
    "alphas1 = np.array(answer1['x'])\n",
    "\n",
    "# ## computing parameters of the line equation\n",
    "w1 = ((y1 * alphas1).T @ G).reshape(-1,1)\n",
    "b1 = y1 - np.dot(G, w1)\n",
    "\n",
    "print(\"Answer 1\")\n",
    "print('Alphas = ',alphas1[alphas1 > 0])\n",
    "print('w1 = ', w1.flatten())\n",
    "print('b1 = ', b1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0:  3.1403e-15 -8.0000e+01  2e+02  1e-01  6e-15\n",
      " 1:  3.1683e-17 -1.4601e+00  2e+00  1e-03  6e-15\n",
      " 2:  2.9897e-19 -1.4603e-02  2e-02  1e-05  7e-15\n",
      " 3:  2.9880e-21 -1.4603e-04  2e-04  1e-07  4e-15\n",
      " 4:  2.9880e-23 -1.4603e-06  2e-06  1e-09  4e-15\n",
      " 5:  2.9880e-25 -1.4603e-08  2e-08  1e-11  8e-15\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "P2 = -1* H.dot(np.linalg.inv(np.dot(G.T, G))).dot(H.T)\n",
    "\n",
    "P2 = cvxopt.matrix(P2)\n",
    "P2 = P2 / np.linalg.norm(P2)\n",
    "q2 = cvxopt.matrix(-np.ones(m2))\n",
    "q2 = q2 / np.linalg.norm(q2)\n",
    "\n",
    "G2 = cvxopt.matrix(np.vstack((np.eye(m2)*-1,np.eye(m2))))\n",
    "h2 = cvxopt.matrix(np.hstack((np.zeros(m2), np.ones(m2) * c2)))\n",
    "\n",
    "\n",
    "A2 = cvxopt.matrix(y2.reshape(1, -1))\n",
    "A2 = A2 / np.linalg.norm(A2)\n",
    "b2 = cvxopt.matrix(np.zeros(1))\n",
    "\n",
    "answer2 = cvxopt.solvers.qp(P2, q2, G2, h2, A2, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 1\n",
      "Alphas =  [2.74762798e-25 1.76887111e-24 7.97228146e-25 1.82635846e-24]\n",
      "w1 =  [-1.93328218e-24  1.02579365e-24 -1.16103751e-24 -1.36845317e-24]\n",
      "b1 =  [-1.]\n"
     ]
    }
   ],
   "source": [
    "## the lagrangian coefficients (named as alpha)\n",
    "alphas2 = np.array(answer2['x'])\n",
    "\n",
    "# ## computing parameters of the line equation\n",
    "w2 = ((y2 * alphas2).reshape(2,4) @ G).reshape(-1,1)\n",
    "b2 = y2[:2] - np.dot(G.T, w2)\n",
    "\n",
    "print(\"Answer 1\")\n",
    "print('Alphas = ',alphas2[alphas2 > 0])\n",
    "print('w1 = ', w2.flatten())\n",
    "print('b1 = ', b2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-SVCR\n",
    "The main concept of K-SVCR proposed by Cecilio Angulo and Andreu Catala in 2003 was to classify more than two classses and it uses the idea of one-vs-one-vs-rest. The difference between this method and the one-vs-one or one-vs-rest is, it will use three classes in each step of support vector evaluation. First it calculates a support vector for class label $+1$, then it calculates another one for label $-1$ and at last for other classes it apply label $0$ for other classes. <br>\n",
    "The equation below would help to better understand this method.(Note that $N$ is the count of our data) \n",
    "\\begin{equation}\n",
    "  f(x_p) =\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if $p=1,\\; 2, ..., n$}\\\\\n",
    "      -1 & \\text{if $p=n+1, \\; n+2, ..., n+m$}\\\\\n",
    "      0 & \\text{if $p=n+m+1, \\; n+m+2, ..., N$}\n",
    "    \\end{cases}       \n",
    "\\end{equation}\n",
    "And the equations for the optimization problem will be\n",
    "\\begin{equation}\n",
    "arg \\: min \\; \\frac{1}{2} ||w||^2\n",
    "\\end{equation}\n",
    "With the equation above we are maximazing the margin of each decesion boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic that needed to be solved is \n",
    "\\begin{equation}\n",
    "arg\\: min \\: L(\\gamma) = \\frac{1}{2} \\gamma^T H \\gamma + c^T \\gamma\n",
    "\\end{equation}\n",
    "and $\\gamma$ is the lagrangian coefficents.\n",
    "\\begin{equation}\n",
    "\\gamma^T = (\\gamma_1, ..., \\gamma_n, \\gamma_{n+1}, ..., \\gamma_{N}) \\\\\n",
    "c^T = (\\frac{-1}{y_1}, \\frac{-1}{y_{n+1}}, \\delta, ..., \\delta)\n",
    "\\end{equation}\n",
    "And $H$, would be the grammian matrix.\n",
    "\\begin{equation}\n",
    "H = \n",
    "\\begin{pmatrix}\n",
    "k(x_i, x_j) & -k(x_i, x_j) & k(x_i, x_j) \\\\\n",
    "-k(x_i, x_j) & k(x_i, x_j) & -k(x_i, x_j) \\\\ \n",
    "k(x_i, x_j) & -k(x_i, x_j) & k(x_i, x_j) \\\\ \n",
    "\\end{pmatrix} \n",
    "= H^T\n",
    "\\end{equation}\n",
    "subject to\n",
    "\\begin{equation}\n",
    "\\gamma_i y_i \\geq 0, \\; i = 1,..., n \\\\\n",
    "\\gamma_i \\geq 0, \\; i = n+1, N \\\\\n",
    "\\sum_{i=1}^{n} \\gamma_i = \\sum_{i = n+1}^{n+m} \\gamma_i - \\sum_{i=n+m+1}^{N} \\gamma_i\n",
    "\\end{equation}\n",
    "\n",
    "So with from the definitions above, it can be found out that the hyperplain decision boundary would be formulated as below\n",
    "\\begin{equation}\n",
    "f(x) = \n",
    "    \\begin{cases}\n",
    "        +1 & \\; \\text{if} \\;\\; \\sum_{i=1}^{SV} v_i k(x_i, x) + b > \\delta \\\\\n",
    "        -1 & \\; \\text{if} \\;\\; \\sum_{i=1}^{SV} v_i k(x_i, x) + b < \\delta \\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "We will implement the three class SVCR, meaning 3-SVCR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('mc-data.csv')\n",
    "df_class0 = df[df['class'] == 0]\n",
    "df_class1 = df[df['class'] == 1]\n",
    "df_class2 = df[df['class'] == 2]\n",
    "\n",
    "plt.scatter(df_class0['x1'], df_class0['x2'])\n",
    "plt.scatter(df_class1['x1'], df_class1['x2'])\n",
    "plt.scatter(df_class2['x1'], df_class2['x2'])\n",
    "plt.legend(['class 0', 'class 1', 'class 2'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the data\n",
    "X = df[['x1','x2']].values\n",
    "Y = df['class']\n",
    "Y = Y.copy()\n",
    "Y[Y == 2] = -1\n",
    "Y = Y.values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_rbf(x1, x2, variance):\n",
    "    \"\"\"\n",
    "    implementation of rbf kernel function\n",
    "\n",
    "    INPUTS:\n",
    "    --------\n",
    "    x1, x2: numpy array of numerical data or scaler numberical data\n",
    "    variance:  the scaler value of the variance, normaly it is float\n",
    "\n",
    "    OUTPUT:\n",
    "    --------\n",
    "    y:  the radias basis kernel function is applied to the input \n",
    "    \"\"\"\n",
    "    # the above value of the division\n",
    "    above = np.abs(x1-x2) ** 2\n",
    "    \n",
    "    below = 2 * (variance ** 2)\n",
    "\n",
    "    y = np.exp(above / below)\n",
    "    return y\n",
    "def kernel_linear(x1, x2):\n",
    "    \"\"\"\n",
    "    linearly dot kernel function\n",
    "    \"\"\"\n",
    "    y = x1.T.dot(x2)\n",
    "    return y\n",
    "\n",
    "\n",
    "m, n = X.shape\n",
    "\n",
    "\n",
    "delta = 0.5\n",
    "\n",
    "## we will devide the c variable into to parts then we concat them\n",
    "## first get the positive labels\n",
    "idx_positive_label = (Y == 1)\n",
    "c1 = -1 / Y[idx_positive_label]\n",
    "\n",
    "## then treat the others as delta\n",
    "idx_non_positive_label = (Y != 1)\n",
    "# Y[idx_non_positive_label] = delta\n",
    "delta_count = idx_non_positive_label.sum()\n",
    "c2 = np.full(shape=delta_count, fill_value=delta)\n",
    "\n",
    "C = np.concatenate((c1, c2), axis=0)\n",
    "\n",
    "## make it a column vector\n",
    "C = C.reshape(-1,1)\n",
    "\n",
    "data_variance = np.var(X)\n",
    "H = np.empty(shape=(m,m))\n",
    "## create the H array\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        # kernel_value = kernel_rbf(X[i], X[j], data_variance) \n",
    "        kernel_value = kernel_linear(X[i], X[j])\n",
    "        H[i][j] = kernel_value\n",
    "        \n",
    "        \n",
    "P = cvxopt.matrix(H)\n",
    "q = cvxopt.matrix(C)\n",
    "\n",
    "G = cvxopt.matrix(np.vstack((np.eye(m),np.eye(m))))\n",
    "h = cvxopt.matrix(np.hstack((np.zeros(m), np.zeros(m))))\n",
    "\n",
    "## the Ax=b will be as below\n",
    "## from the equation 22 we can find out that the, x are the lagrangian variables\n",
    "## labels are the A vector\n",
    "## and the minor change is we must represent the 0 labels as +1\n",
    "\n",
    "## get index of zero labels\n",
    "idx_zero_labels = (Y == 0) \n",
    "Y[idx_zero_labels] = 1\n",
    "\n",
    "A = cvxopt.matrix(Y.reshape(1, -1))\n",
    "A = A / np.linalg.norm(A)\n",
    "b = cvxopt.matrix(np.zeros(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.2395e-01  2.8283e+00  5e+01  8e+00  2e+00\n",
      " 1:  2.0751e+00 -2.4952e+00  1e+01  1e+00  4e-01\n",
      " 2:  1.7341e+00 -7.0884e-02  2e+00  7e-16  2e-14\n",
      " 3:  1.8503e-01 -5.8093e-02  2e-01  3e-16  1e-14\n",
      " 4:  9.7368e-02 -4.8005e-03  1e-01  1e-17  1e-15\n",
      " 5:  7.5818e-03 -5.1229e-04  8e-03  2e-17  7e-16\n",
      " 6:  3.3936e-04 -3.2207e-06  3e-04  1e-18  1e-16\n",
      " 7:  3.7707e-06 -4.3553e-10  4e-06  4e-20  2e-16\n",
      " 8:  3.7703e-08 -4.3555e-14  4e-08  8e-22  1e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "answer = cvxopt.solvers.qp(P, q, G, h, A, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alphas =  [3.23133004e-09 1.08368109e-09 1.92737250e-09 1.11993814e-09\n",
      " 1.66082836e-08 4.50408911e-09 2.58009277e-09 4.22556175e-09\n",
      " 8.91723338e-09 4.81204931e-09 8.29609190e-09 5.17057380e-09\n",
      " 4.52554528e-09 8.40498123e-09 2.70540049e-09 2.40814922e-08\n",
      " 1.23609518e-08 1.55375971e-08 1.70501312e-08 2.16484785e-09\n",
      " 1.50640317e-09]\n",
      "w =  [ 1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      "  1.01300448e-07 -2.77216381e-07  1.01300448e-07 -2.77216381e-07\n",
      " -1.01300448e-07  2.77216381e-07 -1.01300448e-07  2.77216381e-07\n",
      " -1.01300448e-07  2.77216381e-07 -1.01300448e-07  2.77216381e-07\n",
      " -1.01300448e-07  2.77216381e-07 -1.01300448e-07  2.77216381e-07\n",
      " -1.01300448e-07  2.77216381e-07]\n",
      "b =  1.0000264034943436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## the lagrangian coefficients (named as alpha)\n",
    "alphas = np.array(answer['x'])\n",
    "alphas = np.abs(alphas)\n",
    "## computing parameters of the line equation\n",
    "w = ((Y * alphas).T @ X).reshape(-1,1)\n",
    "S = (alphas > 0).flatten()\n",
    "b = Y[S] - np.dot(X[S].flatten(), w)\n",
    "# b = w.T @ np.matrix(X.flatten()).T \n",
    "\n",
    "#Display results\n",
    "print('Alphas = ',alphas[alphas > 0])\n",
    "print('w = ', w.flatten())\n",
    "print('b = ', b[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can see that the solution with it's weights are found."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "29b18f6f7436456ded269b7bb0d23499090a9a566016416baeeafcc627f8cc57"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
